{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbMjKWaXG5lX"
   },
   "source": [
    "## Supervised Learning (Regression)\n",
    "\n",
    "> **Supervised learning** is a type of Machine Learning invlolving learning a function that maps inputs to outputs based on provided  input-output pairs. Learning == Model Training.\n",
    "\n",
    "<img src = \"https://www.researchgate.net/publication/324426321/figure/fig4/AS:614073280507906@1523417887240/Typical-Flow-chart-of-a-supervised-learning-classification-algorithm.png\" width=\"400\">\n",
    "\n",
    "> **Regression** - a type of supervised learning used to predict continuous (dependent) variable given a set of independent variables. \n",
    "\n",
    "> *Mathematical expression*: **Y = Î²o + Î²1X + âˆˆ** where; \n",
    ">   * ...\n",
    "    * Y - Dependent variable(Target, Predicted) - the variable we predict.\n",
    "    * X - Independent variable(predictor) - the variable we use to make a prediction.\n",
    "    * Î²o - This is the intercept term. It is the prediction value you get when X = 0\n",
    "    * Î²1 - This is the slope term. It explains the change in Y when X changes by 1 unit.\n",
    "    * âˆˆ - This represents the residual value, i.e. the difference between actual and predicted values.(error term).\n",
    "    \n",
    ">                       Î²1 = Î£(xi - xmean)(yi-ymean)/ Î£ (xi - xmean)Â² where i= 1 to n (no. of obs.)\n",
    "\n",
    "                        Î²o = ymean - Î²1(xmean)  \n",
    "> expression for multiple regression (when we have more than one predictor variables - typical case): **y = Î²o + Î²1x1 + Î²2x2 + â€¦ + Î²nxn + c.**\n",
    ">   * ...\n",
    "    * Î²n - This is the slope term. It explains the change in Y when Xn changes by 1 unit.\n",
    " \n",
    "    \n",
    "> **Error term minimisation - Ordinary Least Squares(OLS)** - OLS technique tries to reduce the sum of squared errors âˆ‘[Actual(y) - Predicted(y')]Â² by finding the best possible value of regression coefficients (Î²0, Î²1, etc).\n",
    "> Why OLS:\n",
    "1. It uses squared error which has nice mathematical properties, thereby making it easier to differentiate and compute gradient descent.\n",
    "2. OLS is easy to analyze and computationally faster, i.e. it can be quickly applied to data sets having 1000s of features.\n",
    "3. Interpretation of OLS is much easier than other regression techniques.\n",
    "\n",
    "> **Assumptions**:\n",
    "* ...\n",
    "* There exists a **linear** and **additive** relationship between dependent and independent variables. By linear, it means that the change in DV by 1 unit change in IV is constant. By additive, it refers to the effect of X on Y is independent of other variables.\n",
    "* There must be **no correlation** among independent variables. Presence of correlation in independent variables lead to Multicollinearity. If variables are correlated, it becomes extremely difficult for the model to determine the true effect of IVs on DV.\n",
    "* The error terms must possess constant variance. Absence of constant variance leads to **heteroskedestacity**. (Heteroskedastic refers to a condition in which the variance of the residual term, or error term, in a regression model varies widely. \n",
    "* The error terms must be uncorrelated i.e. error at **âˆˆt** must not indicate the error at **âˆˆt+1**. Presence of correlation in error terms is known as **Autocorrelation**. It drastically affects the regression coefficients and standard error values since they are based on the assumption of uncorrelated error terms.\n",
    "* The dependent variable and the error terms must possess a **normal distribution**.\n",
    "\n",
    "> **Examples of regression problems**:\n",
    "* Prediction of house prices.\n",
    "* Prediction of loan eligibility amount.\n",
    "* Salary prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_heESWYG5lm"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhv0yj8jG5lt"
   },
   "source": [
    "#### SLR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 2657,
     "status": "error",
     "timestamp": 1605792927876,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "KXD7EJlXG5l3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/content/trains.csv')\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "executionInfo": {
     "elapsed": 2659,
     "status": "error",
     "timestamp": 1605792927885,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "_9W1XgSQG5mR",
    "outputId": "ab25c7fc-b016-4f69-83b7-0d967acba67d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6208d269f320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2647,
     "status": "aborted",
     "timestamp": 1605792927878,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "bmfw9pUrG5mi"
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2644,
     "status": "aborted",
     "timestamp": 1605792927880,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "sA1eb_HJG5my"
   },
   "outputs": [],
   "source": [
    "# changing date dtype\n",
    "data['Date'] = pd.to_datetime(data.Date)\n",
    "\n",
    "# extracting variables from date column\n",
    "data['Month'] = data['Date'].dt.month.to_list()\n",
    "data['Year'] = data['Date'].dt.year.to_list()\n",
    "data['Day'] = data['Date'].dt.day.to_list()\n",
    "data['WeekOfYear'] = data['Date'].dt.weekofyear.to_list()\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek.to_list()\n",
    "\n",
    "# weekday or not weekday\n",
    "data['weekday'] = 1        # Initialize the column with default value of 1\n",
    "data.loc[data['DayOfWeek'] == 5, 'weekday'] = 0\n",
    "data.loc[data['DayOfWeek'] == 6, 'weekday'] = 0\n",
    "\n",
    "# selection\n",
    "data = data.drop(['Store', 'Date'], axis = 1)\n",
    "data.shape\n",
    "\n",
    "# encoding categorical columns\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# scaling/normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(scaled, columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2637,
     "status": "aborted",
     "timestamp": 1605792927881,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "wkBSSv33G5m8"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2633,
     "status": "aborted",
     "timestamp": 1605792927882,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "rJ8LxM_aG5nM"
   },
   "outputs": [],
   "source": [
    "# seperating data set into dependent and independent variables\n",
    "x = data.drop(['Sales'], axis = 1)\n",
    "y = data[['Sales']]\n",
    "\n",
    "##splitting data set to train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2630,
     "status": "aborted",
     "timestamp": 1605792927883,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "Sp0_lS8FG5nX"
   },
   "outputs": [],
   "source": [
    "# straight-forward modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# creating our model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# model training/fitting\n",
    "regressor = regressor.fit(x_train, y_train)\n",
    "\n",
    "# running predictions\n",
    "y_pred = regressor.predict(x_test)\n",
    "print(y_pred.shape)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2628,
     "status": "aborted",
     "timestamp": 1605792927884,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "Fs3BuQv_G5nh"
   },
   "outputs": [],
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ_DXINcG5nr"
   },
   "source": [
    "#### Cross Validation\n",
    "> Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. \n",
    "\n",
    "<img src = \"data:image/png;base64,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\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1605531343358,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "Sa_V-LxhG5ns",
    "outputId": "4fc24844-2cf9-45b9-a8d8-18cf05f1d608"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3110,
     "status": "ok",
     "timestamp": 1605531345265,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "I0Mt5ncFG5n2",
    "outputId": "ca54176a-c5f2-4e3d-e28b-f1100634d235"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25390416, 0.25606603, 0.25518321, 0.25493593])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use of cross validation (kfolds)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model = LinearRegression()\n",
    "# create a KFold object with 7 splits \n",
    "folds = KFold(n_splits = 4, shuffle = True, random_state = 0)\n",
    "scores = -1 * cross_val_score(model, x_train, y_train, scoring='neg_mean_absolute_error', cv=folds)\n",
    "scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgWYTw9_G5n-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_3Kb0ClG5oH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etaWEP33G5oS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17vcPFING5oZ"
   },
   "source": [
    "#### Decision Trees Regressors\n",
    "\n",
    "> Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A **decision node** has two or more branches. **Leaf node** represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called **root node**. Decision trees can handle both categorical and numerical data. For regression-type problems, the final prediction is usually the average of all of the values contained in the leaf it falls under.\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/410/1*JAEY3KP7TU2Q6HN6LasMrw.png\">\n",
    "\n",
    "> **Entropy** - A measure of the uncertainty associated with a random variable.\n",
    "> **Ei = Î£Pi log2 Pi** where Pi is the probability of randomly picking an element of class i (Measure of impurity).\n",
    "\n",
    "> **Information Gain** - The amount of information gained about a random variable from observing another random variable.  **IG(Y|X) = E(Y) - E(Y|X)**. In other words, IG tells us how many more bits do I need to measure Y, when the information about X is already known. So, in a sense you reduce uncertainty with the additional information available from one random variable. (Measure of purity)\n",
    "<img src = 'https://miro.medium.com/max/700/1*5Bzoc6n44YXGAZtmEWCi9Q.png'>\n",
    "\n",
    "> Steps for constructing a decision tree:\n",
    "1. Calculate **entropy** of the target variable.\n",
    "2. Calculate the **information gain (IG)** by subtracting entropy before the split from the total entropy for the split after spliting the dataeset on different attributes/featuers. \n",
    "3. Choose the attribute with the largest IG as decision node, divide the dataset by its branches and repeat the same process on every branch.\n",
    "4. A branch with 0 entropy is a leaf node. A branch with entropy > 0 needs further split.\n",
    "5. The algorithm is run recursively on non-leaf branches until all decisions have been made. \n",
    "\n",
    "> Assumptions:\n",
    "* In the beginning, the whole training set is considered as the root.\n",
    "* Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.\n",
    "* Records are distributed recursively on the basis of attribute values.\n",
    "* Order to placing attributes as root or internal node of the tree is done by using some statistical approach. \n",
    "\n",
    "> parameters:\n",
    "* criterion - â€œmseâ€, â€œfriedman_mseâ€, â€œmaeâ€; default=â€mseâ€ (measure to measure quality of split).\n",
    "* splitter - â€œbestâ€, â€œrandomâ€, default=â€bestâ€; (strategy used to choose split at each node).\n",
    "* max_depth - default = NOne; how deep should the tree be splitted.\n",
    "* min_samples_split - The minimum number of samples required to split an internal node.\n",
    "\n",
    "> <img src = 'https://miro.medium.com/max/1414/1*nMDP48LmXR0J9R434tSH0A.png' width = \"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26898,
     "status": "ok",
     "timestamp": 1605531374785,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "gFTXfl_oG5oa",
    "outputId": "5314f3b3-2ca7-4273-d57c-b4ff6db128e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30370412, 0.30366601, 0.30426114, 0.30301089, 0.30517469,\n",
       "       0.30510977, 0.30369339])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(random_state = 42)\n",
    "# create a KFold object with 7 splits \n",
    "folds = KFold(n_splits = 7, shuffle = True, random_state = 42)\n",
    "scores = -1 * cross_val_score(model, x_train, y_train, scoring='neg_mean_absolute_error', cv=folds)\n",
    "scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31337,
     "status": "ok",
     "timestamp": 1605531379237,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "AVl7Ddy7G5oj",
    "outputId": "24256c3a-75e9-43a3-b150-beffe44f7dfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.06311527, -0.77680794,  2.67910694, -0.77680794,  2.08263098])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.expm1(y_pred)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zX5_RKWG5or"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GviDW5fyG5oz"
   },
   "source": [
    "### Bayesian Regressors\n",
    "\n",
    "> **Frequentist** approach to regression modeling (one who defines the probability of an event (such as heads in flipping a coin) as the limiting value of its frequency in a large number of trials):\n",
    "\n",
    "<img src = 'https://miro.medium.com/max/445/1*UmoUi8s8awQIEoZeuOQ0rQ.png'>\n",
    "\n",
    "> **Bayesian** approach: someone who tries to determine the probability that a theory is true given the observed data.\n",
    "\n",
    "> **Bayes Theorem**: A mathematical formula for determining conditional probability. **Conditional probability** is the likelihood of an outcome occurring, based on a previous outcome occurring. \n",
    "\n",
    "> Formula:\n",
    "**P(Aâˆ£B) = P(Aâ‹‚B)/P(B) = P(A)â‹…P(Bâˆ£A)/P(B)**\n",
    "\n",
    "> where:\n",
    "* P(A)= The probability of A occurring.\n",
    "* P(B)= The probability of B occurring (Normalizer).\n",
    "* P(Aâˆ£B)=The probability of A given B (Likelihood).\n",
    "* P(Bâˆ£A)= The probability of B given A.\n",
    "* P(Aâ‹‚B))= The probability of both A and B occurring.\n",
    "\n",
    "> **Bayesian approach/inferencing to regression**: \"In the Bayesian viewpoint, we formulate linear regression using probability distributions rather than point estimates. The response, **y**, is not estimated as a single value, but is assumed to be drawn from a probability distribution. The model for Bayesian Linear Regression with the response sampled from a normal distribution is:\"\n",
    "\n",
    "> <img src = 'https://miro.medium.com/max/408/1*JNlUDqc9NWqkT3t9HrGQiw.png' width=\"250\">\n",
    "\n",
    ">The output:\n",
    "* **B** - Weight matrix.\n",
    "* **X** - predictor matrix.\n",
    "* **I** - Identity matrix.\n",
    "* **y** is generated from a normal (Gaussian) Distribution characterized by a mean and variance.\n",
    "* The **mean** for linear regression is the transpose of the weight matrix multiplied by the predictor matrix.\n",
    "* The **variance** is the square of the standard deviation Ïƒ (multiplied by the Identity matrix because this is a multi-dimensional formulation of the model).\n",
    "\n",
    "> The aim of Bayesian Linear Regression is not to find the single â€œbestâ€ value of the model parameters (weights), but rather to determine the posterior distribution for the model parameters. Not only is the response generated from a probability distribution, but the model parameters are assumed to come from a distribution as well. The posterior probability of the model parameters is conditional upon the training inputs and outputs:\n",
    "\n",
    "> <img src = 'https://miro.medium.com/max/377/1*JnXTBQdzzfCaFc1PXVZUQQ.png' width=\"200\">\n",
    "\n",
    "> Here, \n",
    "* P(Î²|y, X) is the posterior probability distribution of the model parameters given the inputs and outputs. \n",
    "* P(y|Î², X), multiplied by the prior probability of the parameters (P(Î²|X) - This is equal to the likelihood of the data) and divided by a normalization constant(P(y|X)) ~ Bayes Theorem.\n",
    "\n",
    "> In contrast to OLS, we have a posterior distribution for the model parameters that is proportional to the likelihood of the data multiplied by the prior probability of the parameters. Here we can observe the two primary benefits of Bayesian Linear Regression.\n",
    "* **Priors** - We have domain knowledge for what the model parameters should look like.\n",
    "* **Posterior** - The result of performing Bayesian Linear Regression is a distribution of possible model parameters based on the data and the prior thus it allows us to quantify uncertainity of our model.\n",
    "\n",
    "#### [Implementation:](https://towardsdatascience.com/bayesian-linear-regression-in-python-using-machine-learning-to-predict-student-grades-part-2-b72059a8ac7e)\n",
    "> Evaluating the posterior distribution for the model parameters is unmanagaable for continuous variables, so we use sampling methods to draw samples from the posterior in order to approximate the posterior. [Monte Carlos methods](https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98) can be applied here. According to wikipedia, **Monte Carlo methods**, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 1482,
     "status": "ok",
     "timestamp": 1605531531970,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "HpqLw_e8G5o1",
    "outputId": "a4d70c30-5167-48cc-b949-aeb2bb8cd64f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Day</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>weekday</th>\n",
       "      <th>StateHoliday_0</th>\n",
       "      <th>StateHoliday_0</th>\n",
       "      <th>StateHoliday_a</th>\n",
       "      <th>StateHoliday_b</th>\n",
       "      <th>StateHoliday_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>403181</th>\n",
       "      <td>1.502791</td>\n",
       "      <td>-1.363330</td>\n",
       "      <td>-2.210440</td>\n",
       "      <td>-0.785400</td>\n",
       "      <td>-0.466372</td>\n",
       "      <td>0.346724</td>\n",
       "      <td>0.215730</td>\n",
       "      <td>-1.104141</td>\n",
       "      <td>0.234490</td>\n",
       "      <td>-1.585611</td>\n",
       "      <td>-0.384596</td>\n",
       "      <td>0.435427</td>\n",
       "      <td>-0.142555</td>\n",
       "      <td>-0.081366</td>\n",
       "      <td>-0.063616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235532</th>\n",
       "      <td>0.000831</td>\n",
       "      <td>-1.363330</td>\n",
       "      <td>-2.210440</td>\n",
       "      <td>-0.785400</td>\n",
       "      <td>2.144211</td>\n",
       "      <td>-1.457193</td>\n",
       "      <td>1.502077</td>\n",
       "      <td>-1.673123</td>\n",
       "      <td>-1.566890</td>\n",
       "      <td>0.630672</td>\n",
       "      <td>-0.384596</td>\n",
       "      <td>-2.296594</td>\n",
       "      <td>7.014824</td>\n",
       "      <td>-0.081366</td>\n",
       "      <td>-0.063616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109696</th>\n",
       "      <td>0.501484</td>\n",
       "      <td>0.025525</td>\n",
       "      <td>0.452399</td>\n",
       "      <td>-0.785400</td>\n",
       "      <td>-0.466372</td>\n",
       "      <td>-0.555234</td>\n",
       "      <td>1.502077</td>\n",
       "      <td>0.944192</td>\n",
       "      <td>-0.458348</td>\n",
       "      <td>0.630672</td>\n",
       "      <td>-0.384596</td>\n",
       "      <td>0.435427</td>\n",
       "      <td>-0.142555</td>\n",
       "      <td>-0.081366</td>\n",
       "      <td>-0.063616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179596</th>\n",
       "      <td>0.501484</td>\n",
       "      <td>0.488476</td>\n",
       "      <td>0.452399</td>\n",
       "      <td>1.273237</td>\n",
       "      <td>-0.466372</td>\n",
       "      <td>-1.156540</td>\n",
       "      <td>1.502077</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>-1.081903</td>\n",
       "      <td>0.630672</td>\n",
       "      <td>-0.384596</td>\n",
       "      <td>0.435427</td>\n",
       "      <td>-0.142555</td>\n",
       "      <td>-0.081366</td>\n",
       "      <td>-0.063616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381533</th>\n",
       "      <td>-1.000476</td>\n",
       "      <td>0.367894</td>\n",
       "      <td>0.452399</td>\n",
       "      <td>1.273237</td>\n",
       "      <td>2.144211</td>\n",
       "      <td>0.346724</td>\n",
       "      <td>0.215730</td>\n",
       "      <td>1.513173</td>\n",
       "      <td>0.511626</td>\n",
       "      <td>0.630672</td>\n",
       "      <td>2.600132</td>\n",
       "      <td>-2.296594</td>\n",
       "      <td>-0.142555</td>\n",
       "      <td>-0.081366</td>\n",
       "      <td>-0.063616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DayOfWeek  Customers  ...  StateHoliday_b  StateHoliday_c\n",
       "403181   1.502791  -1.363330  ...       -0.081366       -0.063616\n",
       "235532   0.000831  -1.363330  ...       -0.081366       -0.063616\n",
       "109696   0.501484   0.025525  ...       -0.081366       -0.063616\n",
       "179596   0.501484   0.488476  ...       -0.081366       -0.063616\n",
       "381533  -1.000476   0.367894  ...       -0.081366       -0.063616\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 5446,
     "status": "error",
     "timestamp": 1605792917470,
     "user": {
      "displayName": "Ada K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4jIOnnAuKbuYGencFM4Z9vB4YM0ZrWcb-Wp00sg=s64",
      "userId": "11129904310952855142"
     },
     "user_tz": -180
    },
    "id": "IbUCxaoLG5o9",
    "outputId": "c6389f4e-d086-4397-cf42-b8af3b3d3817"
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from pymc3 import traceplot\n",
    "\n",
    "# Formula for Bayesian Linear Regression \n",
    "formula = 'Sales ~ ' + ' + '.join(['%s' % variable for variable in x_train.columns])\n",
    "\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqdhHmUDG5pD",
    "outputId": "920a0515-92a5-4a1b-c69d-49dd9ac36877"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [sd, StateHoliday_c, StateHoliday_b, StateHoliday_a, StateHoliday_0[1], StateHoliday_0[0], weekday, WeekOfYear, Day, Year, Month, SchoolHoliday, Promo, Open, Customers, DayOfWeek, Intercept]\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1271/2500 [2:25:08<4:36:40, 13.51s/it]"
     ]
    }
   ],
   "source": [
    "# Context for the model\n",
    "with pm.Model() as normal_model:\n",
    "        \n",
    "    # The prior for the model parameters will be a normal/gaussian distribution.\n",
    "    family = pm.glm.families.Normal()\n",
    "    \n",
    "    # Creating the model requires a formula and data (and optionally a family - to indicate the distribution)\n",
    "    pm.GLM.from_formula(formula, data = x_train, family = family)\n",
    "    \n",
    "    # Perform Markov Chain Monte Carlo sampling\n",
    "    normal_trace = pm.sample(draws=2000, chains = 2, tune = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeyEI_n-G5pK"
   },
   "outputs": [],
   "source": [
    "# posterior plots\n",
    "pm.plot_posterior(normal_trace, figsize = (14, 14), text_size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ezhs4ouK1LA"
   },
   "outputs": [],
   "source": [
    "# Print out the mean variable weight from the trace\n",
    "for variable in normal_trace.varnames:\n",
    "    print('Variable: {:15} Mean weight in model: {:.4f}'.format(variable, np.mean(normal_trace[variable])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKJJ4xUrLPNA"
   },
   "source": [
    "* negatively related to the target variable if weight is negative and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWKUx4OBG5pS"
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate MAE and RMSE\n",
    "def evaluate_prediction(prediction, true):\n",
    "    mae = np.mean(abs(predictions - true))\n",
    "    rmse = np.sqrt(np.mean((predictions - true) ** 2))\n",
    "    \n",
    "    return mae, rmse\n",
    "\n",
    "median_pred = x_train['UWC'].median()\n",
    "median_preds = [median_pred for _ in range(len(x_test))]\n",
    "true = x_test['UWC']\n",
    "\n",
    "# Display mae and rmse\n",
    "mae, rmse = evaluate_prediction(median_preds, true)\n",
    "print('Mean Absolute Error: ',mae)\n",
    "print('Root Mean Square Error:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqA44rSlG5pd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZLzhQ1oG5pl"
   },
   "source": [
    "### Model Evaluation (Loss Functions)\n",
    "\n",
    "> 1. **Mean Square Error, Quadratic loss, L2 Loss** - MSE is the sum of squared distances between our target variable and predicted values. \n",
    "\n",
    "<img src = 'https://miro.medium.com/max/255/1*mlXnpXGdhMefPybSQtRmDA.png' width = '200'>\n",
    "\n",
    "> 2. **Mean Absolute Error, L1 Loss** - sum of absolute differences between our target and predicted variables.\n",
    "\n",
    "<img src = 'https://miro.medium.com/max/257/1*xjarhfIDtRcaNhp7ZEyEdg.png' width = '200'>\n",
    "\n",
    "> PS: using the squared error is easier to solve, but using the absolute error is more robust to outliers.\n",
    "\n",
    "> 3. **Huber Loss, Smooth Mean Absolute Error** - less sensitive to outliers in data than the squared error loss. Itâ€™s also differentiable at 0. Itâ€™s basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, ð›¿ (delta), which can be tuned. Huber loss approaches MSE when ð›¿ ~ 0 and MAE when ð›¿ ~ âˆž (large numbers.)\n",
    "\n",
    "> 4. **Log-Cosh Loss** - smoother than L2. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error. \n",
    "\n",
    "<img src = 'https://miro.medium.com/max/436/1*hj5n5273jYX7rclO7bnfJg.png' width = '200'>\n",
    "\n",
    "> 5. **Quantile Loss** - useful when we are interested in predicting an interval instead of only point predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpQ68Ji4G5po"
   },
   "outputs": [],
   "source": [
    "# mse\n",
    "def mse(true, pred): \n",
    "    return np.sum((true - pred)**2)\n",
    "\n",
    "# mae\n",
    "def mae(true, pred):\n",
    "    return np.sum(np.abs(true - pred))\n",
    " \n",
    "# also available in sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCGLpFdiG5pw"
   },
   "outputs": [],
   "source": [
    "# huber loss\n",
    "def huber(true, pred, delta):\n",
    "    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))\n",
    "    return np.sum(loss)\n",
    "\n",
    "# log cosh loss\n",
    "def logcosh(true, pred):\n",
    "    loss = np.log(np.cosh(pred - true))\n",
    "    return np.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbi7YULkG5p5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMy0lH_mG5p6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZQ_DXINcG5nr",
    "17vcPFING5oZ",
    "eZLzhQ1oG5pl"
   ],
   "name": "3 - Regression.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
