{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB-2FDMwpVFZ"
   },
   "source": [
    "## Classification\n",
    "\n",
    "> Type of supervised learning that specifies a class to which data elements belong.\n",
    "\n",
    "> **binary/binomial classification** - 2 classes involved.\n",
    "\n",
    "> **multi-class classification** - More than 2 classes invovlved. \n",
    "\n",
    "**classification Algorithms**:\n",
    "* Linear Models:\n",
    "     * Logistic Regression.\n",
    "     * Support Vector Machines (SVM).\n",
    "* Non-linear Models:\n",
    "    * Naïve Bayes\n",
    "    * Decision Tree Classification\n",
    "    * Random Forest Classification.\n",
    "\n",
    "* ![spam classification](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2019/11/classification.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1ZbJrd2wXkO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN36ywNitODS"
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    ">  A statistical method for predicting binary classes that computes the probability of an event occurence.\n",
    "\n",
    "> It is a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function. (Can be extended for multi-class classification too).\n",
    "\n",
    "> It transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
    "\n",
    "> Multiple Linear Regression equation: **y = βo + β1x1 + β2x2 + … + βnxn + c.**\n",
    "\n",
    "> Sigmoid Function:\n",
    "     **p = 1/ (1 + e ^ −y)**   \n",
    "\n",
    "> Logistic Regression:\n",
    "     **p = 1/ (1 + e ^ −(βo + β1x1 + β2x2 + … + βnxn + c)**   \n",
    "\n",
    "> Properties of Logistic Regression:\n",
    "* The dependent variable in logistic regression follows Bernoulli Distribution (a special case of the two-point distribution, for which the possible outcomes need not be 0 and 1).\n",
    "* Estimation is done through maximum likelihood and not Least Squares method.\n",
    "\n",
    "> **Sigmoid Function**:\n",
    "* The sigmoid function, also called logistic function gives an ‘S’ shaped curve that can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO. \n",
    "\n",
    "> **MLE**:\n",
    "* The MLE is a \"likelihood\" maximization method, while OLS is a distance-minimizing approximation method. Maximizing the likelihood function determines the parameters that are most likely to produce the observed data. From a statistical point of view, MLE sets the mean and variance as parameters in determining the specific parametric values for a given model. This set of parameters can be used for predicting the data needed in a normal distribution.Linear Regression equation\n",
    "\n",
    "> Types of Logistic Regression:\n",
    "* **Binary Logistic Regression**: The target variable has only two possible outcomes such as Spam or Not Spam, Cancer or No Cancer.\n",
    "* **Multinomial Logistic Regression**: The target variable has three or more nominal categories such as predicting the type of Wine.\n",
    "* **Ordinal Logistic Regression**: the target variable has three or more ordinal categories such as restaurant or product rating from 1 to 5. (Review analysis).\n",
    "\n",
    "> Linear vs Logistic Visual Comparison:\n",
    "![Linear reg vs logistic reg](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281070/linear_vs_logistic_regression_edxw03.png)\n",
    "\n",
    "> **Pros**: doesn't require high computation power, easy to implement, easily interpretable, doesn't require scaling of features. \n",
    "\n",
    "> **Cons**: Logistic regression is not able to handle a large number of categorical features/variables. It is vulnerable to overfitting. Also, can't solve the non-linear problem with the logistic regression that is why it requires a transformation of non-linear features. Logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "8_7DduILAnMT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data.groupby('Outcome'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "KWKW6KEs_41T",
    "outputId": "15312ddb-3f31-4de2-ef23-eb142f5d7f9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading diabetes dataset from kaggle\n",
    "data = pd.read_csv(\"/home/ada/hub/MLClasses/datasets/diabetes.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lT61FcDrDYWK",
    "outputId": "cdcea9d8-7218-430b-dadd-689cbdb34303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Outcome'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBtaAN_WBKNt",
    "outputId": "fded8aca-4efa-442f-9236-72a2b39af509"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 8), (768,))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split dataset into independent and dependent variables\n",
    "X = data.drop(['Outcome'], axis = 1) # Independent/predictor variables\n",
    "y = data['Outcome'] # dependent/target variable\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNXdkgNqBbcb",
    "outputId": "fd646f47-afb8-4b01-b69d-24e78c06b5ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((576, 8), (192, 8), (576,), (192,))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6o64Kc0BgQR",
    "outputId": "bb7bc503-c8f8-4f2a-9def-647b62ec29ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "regressor = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "regressor.fit(x_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWww7ggXExhx"
   },
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9WqlCieE0io"
   },
   "source": [
    "> Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
    "\n",
    "> **Support vectors** are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n",
    "\n",
    "> A **hyperplane** is a decision plane which separates between a set of objects having different class memberships.\n",
    "\n",
    "> A **margin** is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.\n",
    "\n",
    "<img src='https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288454/index2_ub1uzd.png'>\n",
    "\n",
    "> The distance between the either nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum marginal hyperplane in the following steps:\n",
    "\n",
    "> **non-linear and inseparable planes** - In such situation, SVM uses a kernel trick to transform the input space to a higher dimensional space as shown on the right. The data points are plotted on the x-axis and z-axis (Z is the squared sum of both x and y: z=x^2 + y^2). Now you can easily segregate these points using linear separation.\n",
    "\n",
    "<img src='https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index_bnr4rx.png'>\n",
    "\n",
    "> **SVM kernels** - SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space.\n",
    "> ..\n",
    "* **Linear Kernel** A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values. (**K(x, xi) = sum(x * xi)**)\n",
    "\n",
    "* **Polynomial Kernel** A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.Where d is the degree of the polynomial. d=1 is similar to the linear transformation. The degree needs to be manually specified in the learning algorithm. (**K(x,xi) = 1 + sum(x * xi)^d**)\n",
    "\n",
    "* **RBF** can map an input space in infinite dimensional space.gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. (**K(x,xi) = exp(-gamma * sum((x – xi^2))**)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "cancer['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Labels:  ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# print the names of the 13 features\n",
    "print(\"Features: \", cancer.feature_names)\n",
    "\n",
    "# print the label type of cancer('malignant' 'benign')\n",
    "print(\"Labels: \", cancer.target_names)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30), (398,), (171,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAm0nK7QE0rW"
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxCYJVhzE46p"
   },
   "source": [
    "> Naive Bayes classifiers are built on Bayesian classification methods. They rely on Bayes's theorem.We're interested in finding the probability of a label given some observed features, which we can write as P(L | features).It assumes that the effect of a particular feature in a class is independent of other features. \n",
    "\n",
    "> Naive Bayes classifier calculates the probability of an event in the following steps:\n",
    "* Step 1: Calculate the prior probability for given class labels (spam/no spam).\n",
    "* Step 2: Find Likelihood probability with each attribute for each class\n",
    "* Step 3: Put these value in Bayes Formula and calculate posterior probability.\n",
    "* Step 4: See which class has a higher probability, given the input belongs to the higher probability class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)\n",
    "print(\"Predicted Value:\", predicted[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Labels:  ['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "## naive bayes with multiple labels\n",
    "\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# print the names of the 13 features\n",
    "print(\"Features: \", wine.feature_names)\n",
    "\n",
    "# print the label type of wine(class_0, class_1, class_2)\n",
    "print(\"Labels: \", wine.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# print the wine labels (0:Class_0, 1:class_2, 2:class_2)\n",
    "print(wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 0, 1, 0, 0, 1, 0, 2, 2, 2, 2, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)\n",
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOwKgfIXE19U"
   },
   "source": [
    "### Model Evaluation\n",
    "\n",
    "> **Confusion Matrix**: A confusion matrix is a table used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "W93NToWQGrcx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[115,  15],\n",
       "       [ 25,  37]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzxiaRfQHIRU"
   },
   "source": [
    "> **Accuracy**: (True positives + True Negatives) / Total Predictions\n",
    "\n",
    "> **Precision**: Precision is about being precise, i.e.,when a model makes a prediction, how often it is correct. In your prediction case, when the Logistic Regression model predicted patients are going to suffer from diabetes, the patients will suffer/have 71% of the time.\n",
    "\n",
    "> **Recall**: If there are patients who have diabetes in the test set then your Logistic Regression model can identify it 59% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ua7oJKoGG-cX",
    "outputId": "36459624-7617-4da1-e741-2a14a7373a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7916666666666666\n",
      "Precision: 0.7115384615384616\n",
      "Recall: 0.5967741935483871\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26Oxdv5rGwe5"
   },
   "source": [
    "> **ROC Curve** - Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "cBQB8puFHm0M",
    "outputId": "8a129121-1b00-4bf8-a5f0-c4133a67cded"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred_proba = regressor.predict_proba(x_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)  # area under the roc curve\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_zem_ImIB-R"
   },
   "source": [
    "AUC score for the case is 0.86. AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNClBLuOIE7c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QN36ywNitODS",
    "vWww7ggXExhx"
   ],
   "name": "classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
